{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "init_spark.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BDML-Workshop/spark-on-collab/blob/main/init_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYW4RJyeumJz",
        "outputId": "aace815e-504d-4a34-db3d-219b0018ec75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiR9fGWlv8EU"
      },
      "source": [
        "# Running Pyspark in Colab\n",
        "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 3.0.1 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. The tools installation can be carried out inside the Jupyter Notebook of the Colab. Follow the steps to install the dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiI8dXv-v0BW"
      },
      "source": [
        "# install java (in fact collab is a Linux debian like distro)\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzJWMU1hzCbD"
      },
      "source": [
        "the `!` bang operator in the starting of the line is for running a bash script (Linux command) not a python instruction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJIl21BZzziQ",
        "outputId": "98bad603-60cf-4b6a-e713-14f17a7d5fb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# for exemple let see the content of the current directory\n",
        "!ls\n",
        "!echo \"The current dir is $(pwd)\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n",
            "The current dir is /content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-brETQNwtYf"
      },
      "source": [
        "# Download spark\n",
        "![ -f spark-3.0.1-bin-hadoop2.7.tgz ] || wget -q https://mirrors.netix.net/apache/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_DjLNhfyVUD"
      },
      "source": [
        "We download spark with `wget` (a tool for getting things from internet from an url) and I use an binary or (`||`) operator in bash to avoid dowloading the file each time we run this cell. A or B if A is true or is true so bash stop avaluating then no wget"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ur4yWqHyy0cM",
        "outputId": "485d19b7-acdb-4306-d903-73aa4ffb137a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# let see the content again after teh download\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spark-3.0.1-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7p2b_cv0a2E"
      },
      "source": [
        "If you see `spark-3.0.1-bin-hadoop2.7.tgz` : you are on good start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4m_TuGc03CN",
        "outputId": "f6ca3746-8b1e-4c49-9951-4a6e948b3133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#untar it if not already done\n",
        "![ -d spark-3.0.1-bin-hadoop2.7 ] || tar -xzf spark-3.0.1-bin-hadoop2.7.tgz\n",
        "# Again lets look to the new directory content\n",
        "!cp -r spark-3.0.1-bin-hadoop2.7 '/content/drive/My Drive/spark'\n",
        "!ls -l\n",
        "!ls -l '/content/drive/My Drive/spark'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 214788\n",
            "drwx------  5 root root      4096 Oct 25 12:28 drive\n",
            "drwxr-xr-x  1 root root      4096 Oct 14 16:31 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Aug 28 08:10 spark-3.0.1-bin-hadoop2.7\n",
            "-rw-r--r--  1 root root 219929956 Aug 28 09:25 spark-3.0.1-bin-hadoop2.7.tgz\n",
            "total 4\n",
            "drwx------ 13 root root 4096 Oct 25 12:31 spark-3.0.1-bin-hadoop2.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoGdBzwd0oAT"
      },
      "source": [
        "# define some evironement variable diretly with python instruction using the module os\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.1-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kb_TDygr2a_w",
        "outputId": "1d212a34-1582-4506-b983-dabe4c04cc95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# we need now a tool to help python find the spark installation\n",
        "# Happyly their is a module\n",
        "!python3 --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-he2pXCu3pUw"
      },
      "source": [
        "!python -m pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nktpTiHF3zs8",
        "outputId": "11c003c5-8fcf-416d-d496-9cc3cddc8e07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# let test\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"BDML\").master(\"local[*]\").getOrCreate()\n",
        "print(spark)\n",
        "print(spark.sparkContext)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7f928ddd3cc0>\n",
            "<SparkContext master=local[*] appName=BDML>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K4ssd8b4spV"
      },
      "source": [
        "spark.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNtwMOnV6Fa0",
        "outputId": "99eeea22-c5e8-49fc-f448-1ab6fc13ce3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import subprocess\n",
        "output = subprocess.check_output(\"ls -l\", shell=True)\n",
        "print(output.decode('utf-8'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 214784\n",
            "drwxr-xr-x  1 root root      4096 Oct 14 16:31 sample_data\n",
            "drwxr-xr-x 13 1000 1000      4096 Aug 28 08:10 spark-3.0.1-bin-hadoop2.7\n",
            "-rw-r--r--  1 root root 219929956 Aug 28 09:25 spark-3.0.1-bin-hadoop2.7.tgz\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}